{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9691c16c",
      "metadata": {
        "id": "9691c16c"
      },
      "source": [
        "# Assignment 9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f07eee",
      "metadata": {
        "id": "92f07eee"
      },
      "source": [
        "# Please save your file with your name.ipynb and share this Jupter notebook with solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234ec53d",
      "metadata": {
        "id": "234ec53d"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "# What is the role of convolutional layer in CNN in case of image classification model?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Role of Convolutional Layer in CNN for Image Classification\n",
        "The convolutional layer is the fundamental building block of a Convolutional Neural Network (CNN). It is designed to extract features from the input image and plays a crucial role in enabling the network to perform tasks like image classification. Here's a detailed explanation of its role:\n",
        "\n",
        "1. Feature Extraction\n",
        "The convolutional layer applies convolutional filters (kernels) to the input image.\n",
        "These filters scan the image in small regions (receptive fields) and detect specific patterns, such as edges, corners, textures, or shapes.\n",
        "Early layers typically capture low-level features like edges and gradients, while deeper layers capture high-level features like object parts or entire objects.\n",
        "2. Spatial Hierarchy of Features\n",
        "By stacking multiple convolutional layers, CNNs build a hierarchical representation of the input image:\n",
        "Low-level features: Detected by the initial layers (e.g., edges, textures).\n",
        "Mid-level features: Captured by intermediate layers (e.g., shapes, contours).\n",
        "High-level features: Learned by deeper layers (e.g., object parts, semantic patterns).\n",
        "3. Translational Invariance\n",
        "The convolutional process ensures that features are detected regardless of their position in the image. For example, if a cat's ear appears in a different location in two images, the filter still recognizes it.\n",
        "4. Reducing Complexity with Weight Sharing\n",
        "The convolutional layer uses the same filter weights across the entire image (weight sharing). This:\n",
        "Reduces the number of parameters compared to fully connected layers.\n",
        "Makes the network more efficient and less prone to overfitting.\n",
        "5. Dimensionality Reduction\n",
        "The convolution operation, often combined with stride and padding, reduces the spatial dimensions of the feature map.\n",
        "This helps focus on essential features and reduces computational overhead.\n",
        "6. Enabling Non-Linearity\n",
        "After applying the convolution operation, an activation function (like ReLU) is applied element-wise to introduce non-linearity, enabling the network to learn complex patterns.\n",
        "Example in Image Classification\n",
        "For an image classification task (e.g., recognizing whether an image contains a cat or a dog):\n",
        "\n",
        "Initial Layers: Detect low-level features such as edges and corners of the cat or dog.\n",
        "Intermediate Layers: Identify specific shapes like eyes, ears, or tails.\n",
        "Deeper Layers: Recognize the arrangement of these features to classify the entire object as a cat or dog.\n",
        "Key Benefits in Image Classification:\n",
        "Efficient Feature Learning: Automatically learns relevant features without manual engineering.\n",
        "Hierarchical Understanding: Builds a layered understanding of the image, from basic patterns to complex structures.\n",
        "Improved Generalization: Captures universal patterns that generalize well to unseen data.\n",
        "In summary, the convolutional layer is critical for feature extraction, dimensionality reduction, and enabling translational invariance, forming the foundation of accurate image classification models."
      ],
      "metadata": {
        "id": "3-wq2_gR56BG"
      },
      "id": "3-wq2_gR56BG"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IV74c46Dl5nU"
      },
      "id": "IV74c46Dl5nU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8222ac9",
      "metadata": {
        "id": "e8222ac9"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "# Explain the use of pooling layer and dropout layer?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pooling layer is used in Convolutional Neural Networks (CNNs) to reduce the spatial dimensions of feature maps (width and height) while retaining the most important information. This helps in reducing computational complexity, memory usage, and the risk of overfitting.\n",
        "\n",
        "Types of Pooling:\n",
        "\n",
        "Max Pooling:\n",
        "Takes the maximum value from each patch of the feature map.\n",
        "Captures the most prominent features.\n",
        "Helps detect sharp features like edges or textures.\n",
        "\n",
        "Average Pooling:\n",
        "Takes the average of all values in each patch.\n",
        "Captures smoother features and reduces noise.\n",
        "\n",
        "Global Pooling:\n",
        "Reduces the entire feature map to a single value (e.g., Global Max Pooling or Global Average Pooling).\n",
        "Often used as a bridge between convolutional layers and fully connected layers.\n",
        "\n",
        "Benefits:\n",
        "Dimensionality Reduction: Makes the model computationally efficient.\n",
        "Translation Invariance: Helps the network focus on the presence of features rather than their exact location.\n",
        "Feature Extraction: Reduces the feature map size while retaining meaningful information.\n",
        "\n",
        "Dropout Layer\n",
        "The dropout layer is a regularization technique used to prevent overfitting in neural networks by randomly \"dropping out\" (setting to zero) a fraction of neurons during training.\n",
        "\n",
        "How It Works:\n",
        "At each training iteration, a specified percentage of neurons are temporarily deactivated.\n",
        "These neurons do not contribute to the forward pass or backpropagation.\n",
        "This forces the network to learn redundant representations and reduces reliance on specific neurons.\n",
        "\n",
        "Key Parameters:\n",
        "Dropout Rate (p): The fraction of neurons to deactivate (e.g., 0.5 means 50% of neurons are dropped).\n",
        "Benefits:Reduces Overfitting: Prevents the model from becoming too reliant on specific neurons.\n",
        "Encourages Robust Learning: Ensures the network learns distributed representations of data.\n",
        "Improves Generalization: Leads to better performance on unseen data.\n",
        "\n",
        "In Summary:\n",
        "Pooling Layer: Primarily reduces spatial dimensions and retains important features, making the model efficient and invariant to minor translations.\n",
        "\n",
        "Dropout Layer: Helps mitigate overfitting by randomly deactivating neurons during training, improving generalization and robustness."
      ],
      "metadata": {
        "id": "bQJqqj1Z_6k-"
      },
      "id": "bQJqqj1Z_6k-"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3GbC378Zl2Gp"
      },
      "id": "3GbC378Zl2Gp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9e0a71",
      "metadata": {
        "id": "2e9e0a71"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "# What are the applications of CNN?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applications of Convolutional Neural Networks (CNNs)\n",
        "CNNs have revolutionized numerous fields due to their ability to automatically extract features from data, particularly images, with minimal preprocessing. Here are some prominent applications:\n",
        "\n",
        "1. Computer Vision\n",
        "CNNs are widely used for visual data processing and analysis.\n",
        "\n",
        "a. Image Classification\n",
        "Identifying the class of an object in an image.\n",
        "Example: Recognizing cats, dogs, cars, etc., in photos.\n",
        "\n",
        "b. Object Detection\n",
        "Locating and identifying multiple objects in an image.\n",
        "Example: Detecting pedestrians, vehicles, and road signs in autonomous driving.\n",
        "\n",
        "c. Image Segmentation\n",
        "Dividing an image into meaningful regions (pixel-level classification).\n",
        "Example: Medical imaging for tumor segmentation or scene understanding.\n",
        "\n",
        "d. Face Recognition\n",
        "Identifying or verifying a person's identity from facial features.\n",
        "Example: Face ID systems on smartphones or security systems.\n",
        "\n",
        "e. Optical Character Recognition (OCR)\n",
        "Extracting text from images.\n",
        "Example: Digitizing printed documents or reading vehicle license plates.\n",
        "\n",
        "2. Healthcare and Medical Imaging\n",
        "CNNs have become essential tools for diagnosing diseases and analyzing medical images.\n",
        "\n",
        "Disease Detection: Identifying abnormalities in X-rays, MRIs, and CT scans (e.g., cancer, pneumonia).\n",
        "Histopathology: Analyzing tissue samples for diseases.\n",
        "Retinal Analysis: Diagnosing diabetic retinopathy or glaucoma.\n",
        "Endoscopy and Ultrasound: Enhancing image clarity and detecting issues.\n",
        "\n",
        "3. Autonomous Vehicles\n",
        "CNNs play a critical role in enabling self-driving cars by:\n",
        "\n",
        "Lane Detection: Identifying road lanes and markings.\n",
        "Obstacle Detection: Recognizing pedestrians, other vehicles, and road hazards.\n",
        "Traffic Sign Recognition: Reading and understanding traffic signs.\n",
        "\n",
        "4. Natural Language Processing (NLP)\n",
        "Although RNNs and Transformers are common in NLP, CNNs are used for:\n",
        "\n",
        "Text Classification: Sentiment analysis, spam detection, or topic categorization.\n",
        "Document Summarization: Extracting key points from text.\n",
        "Language Translation: Enhancing visual features in image-based text translation.\n",
        "\n",
        "5. Robotics\n",
        "Scene Understanding: Enabling robots to perceive and navigate environments.\n",
        "Grasp Detection: Assisting robotic arms in picking and manipulating objects.\n",
        "\n",
        "6. Retail and E-commerce\n",
        "Product Recommendation: Using visual search to find similar products.\n",
        "Visual Inspection: Detecting defects in manufacturing.\n",
        "Inventory Management: Monitoring stock using image recognition.\n",
        "\n",
        "7. Agriculture\n",
        "Crop Monitoring: Analyzing aerial or satellite images to detect crop health and growth patterns.\n",
        "Weed Detection: Identifying and differentiating weeds from crops for precision farming.\n",
        "Livestock Monitoring: Detecting diseases or behaviors in animals."
      ],
      "metadata": {
        "id": "nM80Pu5MFwaD"
      },
      "id": "nM80Pu5MFwaD"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G2riftNBAwuU"
      },
      "id": "G2riftNBAwuU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0c7824",
      "metadata": {
        "id": "0e0c7824"
      },
      "outputs": [],
      "source": [
        "# Question 4\n",
        "# What are the disadvanatges of traditional RNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disadvantages of Traditional RNNs (Recurrent Neural Networks)\n",
        "While traditional RNNs are powerful for processing sequential data, they come with several limitations that make them less effective for many real-world applications. Below are the key disadvantages:\n",
        "\n",
        "1. Vanishing Gradient Problem\n",
        "In RNNs, gradients are propagated back through time during training.\n",
        "Over long sequences, gradients can shrink exponentially, leading to vanishing gradients, where the network fails to update weights for earlier layers effectively.\n",
        "This limits the RNN’s ability to learn long-term dependencies in the data.\n",
        "\n",
        "2. Exploding Gradient Problem\n",
        "Conversely, gradients can also grow exponentially during backpropagation, leading to unstable training and large weight updates.\n",
        "While gradient clipping can mitigate this issue, it doesn't eliminate the fundamental limitation.\n",
        "\n",
        "3. Difficulty in Learning Long-Term Dependencies\n",
        "Traditional RNNs struggle to retain information over long sequences due to their architecture.\n",
        "As a result, they are ineffective for tasks that require remembering context from far back in time, such as long-form text processing or time series analysis.\n",
        "\n",
        "4. Lack of Parallelization\n",
        "RNNs process input sequentially, meaning the computation for the next time step depends on the previous one.\n",
        "This makes training and inference slower compared to architectures like CNNs or Transformers, which allow for parallel processing.\n",
        "\n",
        "5. Inefficiency with Long Sequences\n",
        "Processing long sequences is computationally expensive due to sequential dependencies.\n",
        "Memory usage increases significantly, as RNNs need to store intermediate states for each time step.\n",
        "\n",
        "6. Limited Flexibility in Handling Variable-Length Sequences\n",
        "Although RNNs can process variable-length input, their performance often deteriorates when the sequence length varies significantly across samples.\n",
        "\n",
        "7. Susceptibility to Overfitting\n",
        "RNNs have a large number of parameters, making them prone to overfitting, especially when the training data is limited.\n",
        "\n",
        "8. Difficulty in Capturing Hierarchical Structures\n",
        "Traditional RNNs cannot effectively model hierarchical or multiscale dependencies in data, such as paragraph structures in text or multi-level patterns in time series.\n",
        "\n",
        "9. Poor Memory Retention\n",
        "The \"memory\" of traditional RNNs is limited due to their recurrent connections and lack of explicit mechanisms to control memory retention or forgetting.\n",
        "\n",
        "10. Challenges in Gradient Descent Optimization\n",
        "The recurrent connections and deep time-step unfolding make optimization harder, requiring advanced techniques like gradient clipping or specialized optimizers.\n",
        "\n",
        "Solutions to Overcome RNN Limitations\n",
        "LSTMs (Long Short-Term Memory Networks):\n",
        "Introduce memory cells and gates to retain information over long sequences and handle vanishing gradients.\n",
        "GRUs (Gated Recurrent Units):\n",
        "Simplify LSTMs while still addressing long-term dependencies effectively.\n",
        "Transformers:\n",
        "Leverage self-attention mechanisms for parallel processing, overcoming the sequential nature of RNNs and enabling efficient learning of long-range dependencies.\n",
        "\n",
        "In summary, traditional RNNs are limited by issues like vanishing gradients, inefficient handling of long-term dependencies, and slow training. These challenges have largely been addressed by more advanced architectures like LSTMs, GRUs, and Transformers, which are now preferred for most sequence modeling tasks."
      ],
      "metadata": {
        "id": "gMSvsTsSIozD"
      },
      "id": "gMSvsTsSIozD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fd1589",
      "metadata": {
        "id": "a8fd1589"
      },
      "outputs": [],
      "source": [
        "# Question 5\n",
        "# Why LSTM is preferred for time series modelling?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM (Long Short-Term Memory) networks are preferred for time series modeling because of their unique ability to handle sequential data and capture temporal dependencies effectively. Here's why they are advantageous:\n",
        "\n",
        "1. Handling Long-Term Dependencies\n",
        "Time series data often exhibit long-term dependencies, where future values depend not only on recent observations but also on events that occurred further back in time.\n",
        "LSTMs use memory cells and gates (input, output, and forget gates) to selectively remember or forget information over long sequences, making them well-suited for capturing these dependencies.\n",
        "\n",
        "2. Mitigating the Vanishing Gradient Problem\n",
        "Traditional RNNs (Recurrent Neural Networks) face challenges like the vanishing gradient problem, where gradients diminish over long sequences, preventing the network from learning effectively.\n",
        "LSTMs overcome this with their internal architecture, allowing gradients to flow unimpeded for longer durations.\n",
        "\n",
        "3. Adaptability to Irregular Patterns\n",
        "Time series data can have varying trends, seasonality, or noise. LSTMs can learn these complex patterns due to their recurrent nature and flexibility in updating or retaining states.\n",
        "\n",
        "4. Multivariate Time Series Capability\n",
        "LSTMs can handle multivariate time series, where multiple variables interact and influence one another. This is crucial for tasks involving multiple interdependent time series data streams.\n",
        "\n",
        "5. Robustness to Missing Data\n",
        "Time series datasets often have missing or irregular data points. LSTMs can interpolate or infer missing values effectively by leveraging temporal correlations.\n",
        "\n",
        "6. Prediction of Sequential Outputs\n",
        "LSTMs can predict not just single values but also sequences of values, making them suitable for forecasting tasks like stock prices, weather, or energy consumption.\n",
        "\n",
        "7. Support for Variable-Length Input\n",
        "Unlike traditional models that may require fixed input sizes, LSTMs can process variable-length sequences, making them versatile for datasets with varying time intervals.\n",
        "\n",
        "Practical Applications:\n",
        "Financial Forecasting: Stock prices, revenue, or sales predictions.\n",
        "\n",
        "Energy Demand Modeling: Predicting power consumption trends.\n",
        "Weather Forecasting: Estimating future climatic conditions.\n",
        "\n",
        "Anomaly Detection: Identifying unusual patterns in time series data, such as fraud detection.\n",
        "\n",
        "In summary, LSTMs are preferred for time series modeling because of their ability to model complex, long-term dependencies while being resilient to challenges like vanishing gradients and missing data."
      ],
      "metadata": {
        "id": "lj0Oap7mKfEs"
      },
      "id": "lj0Oap7mKfEs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43736732",
      "metadata": {
        "id": "43736732"
      },
      "outputs": [],
      "source": [
        "# Question 6\n",
        "# Describe the role of forget gate and cell state in LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Role of Forget Gate and Cell State in LSTM\n",
        "Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) designed to effectively learn long-term dependencies. The forget gate and cell state are central components of an LSTM, enabling it to control the flow of information and address the limitations of traditional RNNs, such as the vanishing gradient problem.\n",
        "\n",
        "1. Forget Gate\n",
        "Purpose:\n",
        "The forget gate determines which information from the previous cell state should be retained or discarded. This is essential for managing the long-term memory of the LSTM.\n",
        "\n",
        "Mechanism:\n",
        "The forget gate is implemented as a sigmoid activation function (𝜎), which outputs values between 0 and 1 for each value in the cell state.\n",
        "1: Completely retain the information.\n",
        "0: Completely forget the information.\n",
        "The forget gate uses the current input (𝑥𝑡 ) and the previous hidden state\n",
        "(ℎ𝑡−1) to compute its decision:\n",
        "𝑓𝑡=𝜎(𝑊𝑓⋅[ℎ𝑡−1,𝑥𝑡]+𝑏𝑓)\n",
        "\n",
        "𝑓𝑡 : Forget gate's output (values between 0 and 1).\n",
        "𝑊𝑓,𝑏𝑓 : Learnable weights and bias.\n",
        "ℎ𝑡−1 : Previous hidden state.\n",
        "𝑥𝑡 : Current input.\n",
        "Role:\n",
        "Filters out unnecessary information in the cell state.\n",
        "Ensures that only relevant information is propagated through time.\n",
        "2. Cell State\n",
        "Purpose:\n",
        "The cell state acts as the long-term memory of the LSTM. It carries information across many time steps, allowing the network to maintain context over long sequences.\n",
        "\n",
        "Mechanism:\n",
        "The cell state is updated in three steps:\n",
        "Forget Unnecessary Information:\n",
        "The forget gate determines which parts of the previous cell state\n",
        "(𝐶𝑡−1 ) should be discarded.\n",
        "𝐶𝑡=𝑓𝑡⋅𝐶𝑡−1\n",
        "\n",
        "Add New Information:\n",
        "The input gate decides which new information to add to the cell state.\n",
        "A candidate update (𝐶~𝑡 ) is computed and scaled by the input gate’s output\n",
        "(𝑖𝑡 ):=Ct +it⋅ C~t​\n",
        "\n",
        "\n",
        "Pass Updated State:\n",
        "The resulting 𝐶𝑡​\n",
        "  is passed to the next time step, enabling the LSTM to retain and update long-term memory dynamically.\n",
        "Role:\n",
        "Maintains a consistent flow of information across time steps.\n",
        "Enables the LSTM to selectively store or discard information, adapting to the needs of the task.\n",
        "Interaction Between Forget Gate and Cell State:\n",
        "The forget gate acts as a filter, deciding what parts of the previous memory (\n",
        "𝐶𝑡−1 ) are no longer relevant.\n",
        "The cell state updates itself by combining the retained memory (from the forget gate) with new information (from the input gate).\n",
        "This interaction allows LSTMs to manage both short-term and long-term dependencies effectively.\n",
        "Why They Are Important:\n",
        "Forget Gate ensures that irrelevant or outdated information does not clutter the memory.\n",
        "Cell State provides a mechanism to store and propagate important information across long sequences.\n",
        "Together, they enable LSTMs to address the vanishing gradient problem and learn dependencies over varying time scales, making them highly effective for tasks like time series forecasting, text generation, and speech recognition."
      ],
      "metadata": {
        "id": "BLaW_cO-OOvx"
      },
      "id": "BLaW_cO-OOvx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9622f35",
      "metadata": {
        "id": "f9622f35"
      },
      "outputs": [],
      "source": [
        "# Question 7\n",
        "# Build time series forecasting model using LSTM for AbbVie Inc ( Symbol: ABBV )\n",
        "# Data file: 'prices_split_adjusted.csv'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6f526d0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6f526d0b",
        "outputId": "99a9f749-60cc-4fe7-d416-054aca738945"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-10-9e9b89fe012d>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-9e9b89fe012d>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    data_info = df.info()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_and_preprocess_data(filepath='content/sample_data/prices_split_adjusted.csv'\n",
        ", symbol='ABBV'):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Display the first few rows and general information about the dataset\n",
        "    data_info = df.info()\n",
        "    data_head = df.head()\n",
        "    data_info, data_head\n",
        "\n",
        "    # Filter for ABBV stock\n",
        "    df_symbol = df[df['symbol'] == symbol].copy()\n",
        "\n",
        "    # Convert date column to datetime\n",
        "    df_symbol['date'] = pd.to_datetime(df_symbol['date'], format='%Y-%m-%d')\n",
        "\n",
        "    # Sort by date\n",
        "    df_symbol = df_symbol.sort_values('date')\n",
        "\n",
        "    # Reset index\n",
        "    df_symbol.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df_symbol\n",
        "    df_symbol.head()\n",
        "\n",
        "# Create sequences for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length)])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Build LSTM model\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        LSTM(50, activation='relu', input_shape=input_shape, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Main forecasting function\n",
        "def forecast_stock_price(file_path, symbol='ABBV', seq_length=10, test_size=0.2):\n",
        "    # Load data\n",
        "    df = load_and_preprocess_data(file_path, symbol)\n",
        "\n",
        "    # Select close prices for forecasting\n",
        "    close_prices = df['close'].values.reshape(-1, 1)\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_prices = scaler.fit_transform(close_prices)\n",
        "\n",
        "    # Create sequences\n",
        "    X, y = create_sequences(scaled_prices, seq_length)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    split = int(len(X) * (1 - test_size))\n",
        "    X_train, X_test = X[:split], X[split:]\n",
        "    y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "    # Build and train the model\n",
        "    model = build_lstm_model((X_train.shape[1], 1))\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    train_predict = model.predict(X_train)\n",
        "    test_predict = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    train_predict = scaler.inverse_transform(train_predict)\n",
        "    y_train_inv = scaler.inverse_transform(y_train)\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    y_test_inv = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    train_rmse = np.sqrt(np.mean((train_predict - y_train_inv)**2))\n",
        "    test_rmse = np.sqrt(np.mean((test_predict - y_test_inv)**2))\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(y_train_inv, label='Actual Train')\n",
        "    plt.plot(train_predict, label='Predicted Train', alpha=0.7)\n",
        "    plt.title('Train Predictions')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(y_test_inv, label='Actual Test')\n",
        "    plt.plot(test_predict, label='Predicted Test', alpha=0.7)\n",
        "    plt.title('Test Predictions')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Train RMSE: {train_rmse}\")\n",
        "    print(f\"Test RMSE: {test_rmse}\")\n",
        "\n",
        "    return model, scaler\n",
        "\n",
        "# Run the forecasting\n",
        "model, scaler = forecast_stock_price('prices_split_adjusted_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "994e4026",
      "metadata": {
        "id": "994e4026"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183a44b6",
      "metadata": {
        "id": "183a44b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf4ee45",
      "metadata": {
        "id": "aaf4ee45"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5303135",
      "metadata": {
        "id": "d5303135"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42798e80",
      "metadata": {
        "id": "42798e80"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8714b9e6",
      "metadata": {
        "id": "8714b9e6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73474fb2",
      "metadata": {
        "id": "73474fb2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}